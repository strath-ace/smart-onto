{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Fine-tune_SpaceTransformer.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Unra_3rMmKt6"
      },
      "source": [
        "## Introduction\n",
        "This notebook was created as a tutorial for the puplication \"Space transformers: language modeling for space systems\". It uses the further pre-trained SpaceTransformer models (SpaceBERT, SpaceSciBERT, SpaceRoBERTa) and fine-tunes them on the Concept Recognition task from the paper. \n",
        "\n",
        "---\n",
        "\n",
        "If you use the models for your experiments please cite: \n",
        "\n",
        "\n",
        "Berquand, A., Darm, P., & Riccardi, A. (2021). Space transformers: language modeling for space systems. IEEE Access, 9, 133111-133122. https://doi.org/10.1109/ACCESS.2021.3115659\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y8whZEfCn_nf",
        "cellView": "form"
      },
      "source": [
        "# @title Licensed under the MIT License\n",
        "\n",
        "# Copyright (c) 2021 Paul Darm\n",
        "\n",
        "# Permission is hereby granted, free of charge, to any person obtaining a copy\n",
        "# of this software and associated documentation files (the \"Software\"), to deal\n",
        "# in the Software without restriction, including without limitation the rights\n",
        "# to use, copy, modify, merge, publish, distribute, sublicense, and/or sell\n",
        "# copies of the Software, and to permit persons to whom the Software is\n",
        "# furnished to do so, subject to the following conditions:\n",
        "\n",
        "# The above copyright notice and this permission notice shall be included in\n",
        "# all copies or substantial portions of the Software.\n",
        "\n",
        "# THE SOFTWARE IS PROVIDED \"AS IS\", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR\n",
        "# IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,\n",
        "# FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE\n",
        "# AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER\n",
        "# LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,\n",
        "# OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN\n",
        "# THE SOFTWARE."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c9gaVnIPadbo"
      },
      "source": [
        "Google Colab offers free GPUs and TPUs. Since we'll be training a large neural network it's best to take advantage of this (in this case we'll attach a GPU), otherwise training will take a very long time.\n",
        "\n",
        "A GPU can be added by going to the menu and selecting:\n",
        "\n",
        "`Edit ðŸ¡’ Notebook Settings ðŸ¡’ Hardware accelerator ðŸ¡’ (GPU)`\n",
        "\n",
        "Then run the following cell to confirm that the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9cLcVup0N774",
        "cellView": "form",
        "outputId": "b2396fc1-8804-4e06-a5ec-6b89c0582a04"
      },
      "source": [
        "#@title Install necessary libraries and modules when running notebook on Google Colab\n",
        "!pip install transformers\n",
        "\n",
        "\n",
        "from transformers import AutoTokenizer, AutoModel, AutoModelForTokenClassification\n",
        "from transformers import AdamW\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import TensorDataset, DataLoader, RandomSampler, SequentialSampler\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.model_selection import ShuffleSplit \n",
        "from sklearn.metrics import precision_recall_fscore_support\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import f1_score\n",
        "from sklearn.metrics import recall_score#\n",
        "from sklearn.metrics import precision_score\n",
        "\n",
        "import spacy\n",
        "import json\n",
        "import time\n",
        "import pandas as pd\n",
        "import warnings"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.7/dist-packages (4.11.3)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers) (21.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: huggingface-hub>=0.0.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.19)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers) (0.0.46)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers) (3.3.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (1.19.5)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers) (0.10.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers) (4.8.1)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers) (4.62.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from huggingface-hub>=0.0.17->transformers) (3.7.4.3)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers) (2.4.7)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers) (3.6.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2021.5.30)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.0.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WNYFbdLYFSTh"
      },
      "source": [
        "## Load data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "RxZ3HcgZIdiU",
        "outputId": "848dfe81-b347-4508-bcc5-24a0f7015e5d"
      },
      "source": [
        "## Load CR dataset from GitHub\n",
        "!wget https://raw.githubusercontent.com/strath-ace/smart-nlp/master/SpaceTransformers/CR/CR_ECSS_dataset.json\n",
        "dataset = pd.read_json('/content/CR_ECSS_dataset.json')\n",
        "dataset"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2021-10-20 14:18:30--  https://raw.githubusercontent.com/strath-ace/smart-nlp/master/SpaceTransformers/CR/CR_ECSS_dataset.json\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1645094 (1.6M) [text/plain]\n",
            "Saving to: â€˜CR_ECSS_dataset.json.2â€™\n",
            "\n",
            "\rCR_ECSS_dataset.jso   0%[                    ]       0  --.-KB/s               \rCR_ECSS_dataset.jso 100%[===================>]   1.57M  --.-KB/s    in 0.07s   \n",
            "\n",
            "2021-10-20 14:18:30 (22.6 MB/s) - â€˜CR_ECSS_dataset.json.2â€™ saved [1645094/1645094]\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sentence_id</th>\n",
              "      <th>words</th>\n",
              "      <th>labels</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>It</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>shall</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>be</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>demonstrated</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>that</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31438</th>\n",
              "      <td>874</td>\n",
              "      <td>energy</td>\n",
              "      <td>Space Environment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31439</th>\n",
              "      <td>874</td>\n",
              "      <td>deposition</td>\n",
              "      <td>Space Environment</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31440</th>\n",
              "      <td>874</td>\n",
              "      <td>,</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31441</th>\n",
              "      <td>874</td>\n",
              "      <td>e</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31442</th>\n",
              "      <td>874</td>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>31443 rows Ã— 3 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "       sentence_id         words             labels\n",
              "0                0            It                  O\n",
              "1                0         shall                  O\n",
              "2                0            be                  O\n",
              "3                0  demonstrated                  O\n",
              "4                0          that                  O\n",
              "...            ...           ...                ...\n",
              "31438          874        energy  Space Environment\n",
              "31439          874    deposition  Space Environment\n",
              "31440          874             ,                  O\n",
              "31441          874             e                  O\n",
              "31442          874             .                  O\n",
              "\n",
              "[31443 rows x 3 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J6qJnZ8oFesF",
        "outputId": "c7437fee-3899-41ed-82b0-2d23d9da8fc2"
      },
      "source": [
        "## Unique values of labels (tags) in dataset \n",
        "tag_vals = dataset['labels'].unique() \n",
        "##Dict to transform numbers back into their original tags  'Quality control' --> 8 / 'O' --> 4\n",
        "tag2idx = {}\n",
        "for count,  tag in enumerate(tag_vals):\n",
        "   tag2idx[tag] = count\n",
        "print(tag2idx)\n",
        "## Dict to transform numbers back into their original tags 4 --> 'O', 2 --> 'Space Environment'\n",
        "tag2name={tag2idx[key] : key for key in tag2idx.keys()}\n",
        "\n",
        "## Add additional tag for \"None\" label of Pytorch\n",
        "## --> see https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities\n",
        "tag2name[-100]= \"None\"\n",
        "print(tag2name)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'O': 0, 'Cleanliness': 1, 'Materials / EEEs': 2, 'Nonconformity': 3, 'System engineering': 4, 'Quality control': 5, 'Measurement': 6, 'Parameter': 7, 'GN&C': 8, 'Project Scope': 9, 'OBDH': 10, 'Power': 11, 'Structure & Mechanism': 12, 'Thermal': 13, 'Telecom.': 14, 'Space Environment': 15, 'Project Organisation / Documentation': 16, 'Safety / Risk (Control)': 17, 'Propulsion': 18}\n",
            "{0: 'O', 1: 'Cleanliness', 2: 'Materials / EEEs', 3: 'Nonconformity', 4: 'System engineering', 5: 'Quality control', 6: 'Measurement', 7: 'Parameter', 8: 'GN&C', 9: 'Project Scope', 10: 'OBDH', 11: 'Power', 12: 'Structure & Mechanism', 13: 'Thermal', 14: 'Telecom.', 15: 'Space Environment', 16: 'Project Organisation / Documentation', 17: 'Safety / Risk (Control)', 18: 'Propulsion', -100: 'None'}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cZefYNzXw2X"
      },
      "source": [
        "## Data pre-processing "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zreMLOfkOigd"
      },
      "source": [
        "def tokenize_and_align_labels(examples, labels, tokenizer):\n",
        "    \"\"\"\n",
        "    Taken and adapted  from: https://github.com/huggingface/notebooks/blob/master/examples/token_classification.ipynb\n",
        "    Adjust the list of lables to the word-piece tokenisation of BERT-type models  \n",
        "    [polyethylene] --> [poly, ##eth, ##yle, ##ne]\n",
        "    [2]            --> [2, -100,-100,-100]\n",
        "\n",
        "    \"\"\"\n",
        "\n",
        "    tokenized_inputs = tokenizer([example for example in examples], padding=True, truncation=False,is_split_into_words=True)\n",
        "                        \n",
        "    word_piece_labels = []\n",
        "    label_all_tokens = True\n",
        "    for i, label in enumerate(labels):\n",
        "        word_ids = tokenized_inputs.word_ids(batch_index=i)\n",
        "        previous_word_idx = None\n",
        "        label_ids = []\n",
        "        for word_idx in word_ids:\n",
        "            # Special tokens have a word id that is None. We set the label to -100 so they are automatically\n",
        "            # ignored in the loss function.\n",
        "            # --> see https://huggingface.co/transformers/custom_datasets.html#token-classification-with-w-nut-emerging-entities\n",
        "            if word_idx is None:\n",
        "                label_ids.append(-100)\n",
        "            # We set the label for the first token of each word.\n",
        "            elif word_idx != previous_word_idx:\n",
        "                label_ids.append(label[word_idx])\n",
        "            # For the other tokens in a word, we set the label to either the current label or -100, depending on\n",
        "            # the label_all_tokens flag.\n",
        "            else:\n",
        "                label_ids.append(label[word_idx] if label_all_tokens else -100)\n",
        "            previous_word_idx = word_idx\n",
        "\n",
        "        word_piece_labels.append(label_ids)\n",
        "\n",
        "    tokenized_inputs[\"labels\"] = word_piece_labels\n",
        "    return tokenized_inputs"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NXg5cOmsRH_O"
      },
      "source": [
        "## Tokenisation\n",
        "\n",
        "The single words in the requirements need to be \"tokenised\", before we can use them as input for the model. \n",
        "\n",
        "Tokenisation consists of two steps: \n",
        "\n",
        "\n",
        "\n",
        "1.   Transforming words into single tokens the models vocabulary (BERT models --> word-piece tokenisation, polyethylene --> poly ##eth ##yle ##ne) \n",
        "2.   Transform the tokens into their corresponing IDs of the vocabulary ( poly ##eth ##yle ##ne --> 26572, 11031, 12844, 2638)\n",
        "\n",
        "Therefore, we need to download the respective tokeniser from Huggingface (BERT - for SpaceBERT, SciBERT - for SpaceSciBERT, RoBERTa for SpaceRoBERTa).\n",
        "We do this by instantiating our tokenizer with the AutoTokenizer.from_pretrained .\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AqUZXt74Swb0",
        "outputId": "749fccbf-ae22-4e44-9397-a5a138273030"
      },
      "source": [
        "tokenizer('polyethylene')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'input_ids': [101, 26572, 11031, 12844, 2638, 102], 'token_type_ids': [0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1]}"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zpHfOAIvfyCM",
        "outputId": "efc27b66-152b-4d60-fc1b-adaac8646d3a"
      },
      "source": [
        "#=======================================\n",
        "#          DATA PREPARATION \n",
        "#=======================================\n",
        "\n",
        "## Different Tokenizers\n",
        "#tokenizer = AutoTokenizer.from_pretrained('bert-base-uncased')  ##SpaceBERT\n",
        "#tokenizer = AutoTokenizer.from_pretrained('allenai/scibert_scivocab_uncased') ## SpaceSciBERT\n",
        "tokenizer = AutoTokenizer.from_pretrained('roberta-base',add_prefix_space=True) ## SpaceRoBERTa\n",
        "\n",
        "label_all_tokens = True\n",
        "\n",
        "sentences = [[word for word in dataset[dataset['sentence_id']==i]['words'].values] for i in dataset['sentence_id'].unique()]\n",
        "labels = [[tag2idx[label] for label in dataset[dataset['sentence_id']==i]['labels'].values] for i in dataset['sentence_id'].unique()]\n",
        "\n",
        "encoded_input = tokenize_and_align_labels(sentences, labels, tokenizer)\n",
        "\n",
        "input_ids = encoded_input[\"input_ids\"]\n",
        "\n",
        "attention_masks = encoded_input[\"attention_mask\"]\n",
        "\n",
        "labels = encoded_input['labels']\n",
        "\n",
        "for i in range(0,5):\n",
        "        print(\"No.%d,len:%d\"%(i,len(input_ids[i])))\n",
        "        print(\"texts:%s\"%(\" \".join(i for i in tokenizer.tokenize(tokenizer.decode(input_ids[i])))))\n",
        "        print(\"No.%d,len:%d\"%(i,len(labels[i])))\n",
        "        #Reduced Tags\n",
        "        print(\"lables:%s\"%(\" \".join(tag2name[j] for j in labels[i])))\n",
        "\n",
        "\n",
        "tr_inputs, val_inputs, tr_tags, val_tags,tr_masks, val_masks = train_test_split(input_ids, labels,attention_masks, \n",
        "                                                     random_state=4, test_size=0.213)\n",
        "                                                           \n",
        "\n",
        "\n",
        "tr_inputs = torch.as_tensor(tr_inputs)\n",
        "val_inputs = torch.as_tensor(val_inputs)\n",
        "tr_tags = torch.as_tensor(tr_tags)\n",
        "val_tags = torch.as_tensor(val_tags)\n",
        "tr_masks = torch.as_tensor(tr_masks)\n",
        "val_masks = torch.as_tensor(val_masks)\n",
        "\n",
        "batch_num = 16\n",
        "\n",
        "# Only set token embedding, attention embedding, no segment embedding\n",
        "train_data = TensorDataset(tr_inputs, tr_masks, tr_tags)\n",
        "train_sampler = RandomSampler(train_data)\n",
        "# Drop last can make batch training better for the last one\n",
        "train_dataloader = DataLoader(train_data, sampler=train_sampler, batch_size=batch_num,drop_last=False)\n",
        "\n",
        "valid_data = TensorDataset(val_inputs, val_masks, val_tags)\n",
        "valid_sampler = SequentialSampler(valid_data)\n",
        "valid_dataloader = DataLoader(valid_data, sampler=valid_sampler, batch_size=batch_num)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "No.0,len:497\n",
            "texts:<s> Ä It Ä shall Ä be Ä demonstrated Ä that Ä no Ä additional Ä contamination Ä is Ä introduced Ä during Ä the Ä handling Ä process . Ä  ÄŠ Ä  Ä  Ä  Ä  Ä NOTE Ä 1 Ä Cont amination Ä can Ä be Ä avoided Ä by Ä using Ä tw eez ers Ä and Ä clean Ä gloves , Ä and Ä ensuring Ä that Ä gloves Ä and Ä chemicals Ä are Ä compatible Ä  ÄŠ Ä  Ä  Ä  Ä  Ä NOTE Ä 2 Ä Typically Ä used Ä gloves Ä are Ä of Ä powder Ä - Ä free Ä nylon , Ä nit ri le , Ä latex , Ä l int Ä - Ä free Ä cotton . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "No.0,len:497\n",
            "lables:None O O O O O O O Cleanliness O O O O O O O O O O O O O O O Cleanliness Cleanliness O O O O O O O O O Cleanliness Cleanliness O O O O O O Materials / EEEs O O O O O O O O O O O O O O O Materials / EEEs Materials / EEEs Materials / EEEs Materials / EEEs O Materials / EEEs Materials / EEEs Materials / EEEs O Materials / EEEs O Cleanliness Cleanliness Cleanliness Cleanliness Cleanliness O None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None\n",
            "No.1,len:497\n",
            "texts:<s> Ä Physical Ä damage Ä shall Ä be Ä avoided Ä by Ä packing Ä the Ä poly ethyl ene Ä or Ä poly prop ylene Ä wrapped Ä work pieces Ä in Ä clean , Ä dust - Ä and Ä l int Ä free Ä material . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "No.1,len:497\n",
            "lables:None Nonconformity Nonconformity O O O O O O Materials / EEEs Materials / EEEs Materials / EEEs O Materials / EEEs Materials / EEEs Materials / EEEs O System engineering System engineering O Cleanliness O Cleanliness Cleanliness O Cleanliness Cleanliness Cleanliness O O None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None\n",
            "No.2,len:497\n",
            "texts:<s> Ä Sam ples Ä shall Ä only Ä be Ä handled Ä with Ä clean Ä nylon Ä or Ä l int Ä free Ä gloves . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "No.2,len:497\n",
            "lables:None Quality control Quality control O O O O O O Materials / EEEs O Cleanliness Cleanliness Cleanliness Cleanliness O None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None\n",
            "No.3,len:497\n",
            "texts:<s> Ä The Ä supplier Ä shall Ä avoid Ä physical Ä damage Ä by Ä packing Ä the Ä poly ethyl ene Ä or Ä poly prop ylene Ä - Ä wrapped Ä work pieces Ä in Ä clean , Ä dust - Ä and Ä l int Ä - Ä free Ä material . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "No.3,len:497\n",
            "lables:None O O O O Nonconformity Nonconformity O O O Materials / EEEs Materials / EEEs Materials / EEEs O Materials / EEEs Materials / EEEs Materials / EEEs O O O O O O O O O O Cleanliness Cleanliness Cleanliness Cleanliness Cleanliness O None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None\n",
            "No.4,len:497\n",
            "texts:<s> Ä The Ä supplier Ä shall Ä remove Ä any Ä particles Ä or Ä contamination Ä visible Ä on Ä the Ä outer Ä insulation Ä under Ä a Ä magnification Ä of Ä ÃƒÄ¹ 10 Ä with Ä a Ä clean , Ä l int Ä - Ä free Ä cloth . </s> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad> <pad>\n",
            "No.4,len:497\n",
            "lables:None O O O O O Cleanliness O Cleanliness O O O Materials / EEEs Materials / EEEs O O O O Measurement Measurement O O O O Cleanliness Cleanliness Cleanliness Cleanliness Cleanliness O None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None None\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cnR_nuGFViPS"
      },
      "source": [
        "## Model Loading \n",
        "After data preparation we load a pre-trained SpaceTransformer, for fine-tuning it on CR. Since our tasks is about classifying single words (tokens), we use the AutoModelForTokenClassification class. \n",
        "\n",
        "Like with the tokenizer, the from_pretrained method will download and cache the model for us. The only thing we have to specify is the number of labels for our problem (which we can get from our tag2idx dictionary).\n",
        "\n",
        "When loading the model, a warning is telling us we are throwing away some weights (the vocab_transform and vocab_layer_norm layers) and randomly initializing some other (the token classifier layers). This is absolutely normal in this case, because we are removing the head used to pretrain the model on a masked language modeling objective and replacing it with a new head for which we don't have pretrained weights, so the library warns us we should fine-tune this model before using it for inference, which is exactly what we are going to do."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eRs3qd5DVSRZ",
        "outputId": "daf0b837-dd1d-4cbe-c0b6-69a64bf1008d"
      },
      "source": [
        "## Load pre-trained SpaceBERT from Huggingface for fine-tuning (exchange for any model you want to finetune)\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(\"icelab/spaceroberta\", num_labels=len(tag2idx))\n",
        "\n",
        "## Train with GPU, if available\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda()\n",
        "    if n_gpu >1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at icelab/spaceroberta were not used when initializing RobertaForTokenClassification: ['lm_head.layer_norm.bias', 'lm_head.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight']\n",
            "- This IS expected if you are initializing RobertaForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForTokenClassification were not initialized from the model checkpoint at icelab/spaceroberta and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZx6eoQwX5XD"
      },
      "source": [
        "## Train script \n",
        "\n",
        "This training loop is heavily influenced by Chris McCormicks notebook \"BERT Fine-Tuning Sentence Classification\". \n",
        "\n",
        "https://colab.research.google.com/drive/1pTuQhug6Dhl9XalKB0zUGf4FIdYFlpcX#scrollTo=GI0iOY8zvZzL\n",
        "\n",
        "**From the notebook:**\n",
        "\n",
        "\"> *Thank you to [Stas Bekman](https://ca.linkedin.com/in/stasbekman) for contributing the insights and code for using validation loss to detect over-fitting!*\n",
        "\n",
        "**Training:**\n",
        "- Unpack our data inputs and labels\n",
        "- Load data onto the GPU for acceleration\n",
        "- Clear out the gradients calculated in the previous pass. \n",
        "    - In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out.\n",
        "- Forward pass (feed input data through the network)\n",
        "- Backward pass (backpropagation)\n",
        "- Tell the network to update parameters with optimizer.step()\n",
        "- Track variables for monitoring progress\n",
        "\n",
        "**Evalution:**\n",
        "- Unpack our data inputs and labels\n",
        "- Load data onto the GPU for acceleration\n",
        "- Forward pass (feed input data through the network)\n",
        "- Compute loss on our validation data and track variables for monitoring progress\" \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KgGq0LDWNhsv",
        "outputId": "93036caa-da3e-4fe6-f367-5c1a3b6382d3"
      },
      "source": [
        "import time\n",
        "import datetime\n",
        "import random\n",
        "import numpy as np\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Takes a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second.\n",
        "    elapsed_rounded = int(round((elapsed)))\n",
        "    \n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds=elapsed_rounded))\n",
        "\n",
        "#\n",
        "# This training code is based on the `run_glue.py` script here:\n",
        "# https://github.com/huggingface/transformers/blob/5bfcd0485ece086ebcbed2d008813037968a9e58/examples/run_glue.py#L128\n",
        "\n",
        "# Set the seed value all over the place to make this reproducible.\n",
        "seed_val = 42\n",
        "random.seed(seed_val)\n",
        "np.random.seed(seed_val)\n",
        "torch.manual_seed(seed_val)\n",
        "torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "#=======================================\n",
        "#          Train parameters\n",
        "#=======================================\n",
        "\n",
        "## determines if all layers are fine_tuned or just the last, newly initialized ones \n",
        "FULL_FINETUNING = True\n",
        "\n",
        "\n",
        "if FULL_FINETUNING:\n",
        "    # Fine-tune model all layer parameters\n",
        "    param_optimizer = list(model.named_parameters())\n",
        "    #no_decay = ['bias', 'gamma', 'beta']\n",
        "    no_decay = []\n",
        "    optimizer_grouped_parameters = [\n",
        "        {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.00, 'correct_bias' : False},\n",
        "        {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)],\n",
        "         'weight_decay_rate': 0.0,'correct_bias' : False}\n",
        "    ] \n",
        "else:\n",
        "    # Only fine tune classifier parameters\n",
        "    param_optimizer = list(model.classifier.named_parameters()) \n",
        "    optimizer_grouped_parameters = [{\"params\": [p for n, p in param_optimizer]}]\n",
        "optimizer = AdamW(optimizer_grouped_parameters, lr=3e-5) ## lr = learning rate of the model: default value 5e-5\n",
        "\n",
        "# Number of training epochs. The BERT authors recommend between 2 and 4. \n",
        "epochs = 4\n",
        "\n",
        "\n",
        "\n",
        "# Total number of training steps is [number of batches] x [number of epochs]. \n",
        "# (Note that this is not the same as the number of training samples).\n",
        "total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "# Create the learning rate scheduler.\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, \n",
        "                                            num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                            num_training_steps = total_steps)\n",
        "\n",
        "# We'll store a number of quantities such as training and validation loss, \n",
        "# validation accuracy, and timings.\n",
        "training_stats = []\n",
        "train_loss = []\n",
        "\n",
        "# Measure the total training time for the whole run.\n",
        "total_t0 = time.time()\n",
        "\n",
        "# For each epoch...\n",
        "for epoch_i in range(0, epochs):\n",
        "    \n",
        "    # ========================================\n",
        "    #               Training\n",
        "    # ========================================\n",
        "    \n",
        "    # Perform one full pass over the training set.\n",
        "\n",
        "    print(\"\")\n",
        "    print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "    print('Training...')\n",
        "\n",
        "    # Measure how long the training epoch takes.\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Reset the total loss for this epoch.\n",
        "    total_train_loss = 0\n",
        "\n",
        "    # Put the model into training mode. Don't be mislead--the call to \n",
        "    # `train` just changes the *mode*, it doesn't *perform* the training.\n",
        "    # `dropout` and `batchnorm` layers behave differently during training\n",
        "    # vs. test (source: https://stackoverflow.com/questions/51433378/what-does-model-train-do-in-pytorch)\n",
        "    model.train()\n",
        "\n",
        "    # For each batch of training data...\n",
        "    for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "        # `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "\n",
        "        # Always clear any previously calculated gradients before performing a\n",
        "        # backward pass. PyTorch doesn't do this automatically because \n",
        "        # accumulating the gradients is \"convenient while training RNNs\". \n",
        "        # (source: https://stackoverflow.com/questions/48001598/why-do-we-need-to-call-zero-grad-in-pytorch)\n",
        "        model.zero_grad()        \n",
        "\n",
        "        # Perform a forward pass (evaluate the model on this training batch).\n",
        "        # The documentation for this `model` function is here: \n",
        "        # https://huggingface.co/transformers/v2.2.0/model_doc/bert.html#transformers.BertForSequenceClassification\n",
        "        # It returns different numbers of parameters depending on what arguments\n",
        "        # arge given and what flags are set. For our useage here, it returns\n",
        "        # the loss (because we provided labels) and the \"logits\"--the model\n",
        "        # outputs prior to activation.\n",
        "        outputs = model(b_input_ids, \n",
        "                             token_type_ids=None, \n",
        "                             attention_mask=b_input_mask, \n",
        "                             labels=b_labels)\n",
        "        loss, logits =  outputs[:2]\n",
        "                           \n",
        "\n",
        "        # Accumulate the training loss over all of the batches so that we can\n",
        "        # calculate the average loss at the end. `loss` is a Tensor containing a\n",
        "        # single value; the `.item()` function just returns the Python value \n",
        "        # from the tensor.\n",
        "        total_train_loss += loss.item()\n",
        "\n",
        "        # Perform a backward pass to calculate the gradients.\n",
        "        loss.backward()\n",
        "\n",
        "        # Clip the norm of the gradients to 1.0.\n",
        "        # This is to help prevent the \"exploding gradients\" problem.\n",
        "        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "        # Update parameters and take a step using the computed gradient.\n",
        "        # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "        # modified based on their gradients, the learning rate, etc.\n",
        "        optimizer.step()\n",
        "\n",
        "        # Update the learning rate.\n",
        "        scheduler.step()\n",
        "\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "    \n",
        "    # Measure how long this epoch took.\n",
        "    training_time = format_time(time.time() - t0)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "    print(\"  Training epoch took: {:}\".format(training_time))\n",
        "    train_loss.append(avg_train_loss)\n",
        "\n",
        "    # ========================================\n",
        "    #               Validation\n",
        "    # ========================================\n",
        "    # After the completion of each training epoch, measure our performance on\n",
        "    # our validation set.\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Running Validation...\")\n",
        "\n",
        "    t0 = time.time()\n",
        "\n",
        "    # Put the model in evaluation mode--the dropout layers behave differently\n",
        "    # during evaluation.\n",
        "    model.eval()\n",
        "\n",
        "    # Tracking variables \n",
        "    # total_eval_accuracy = 0\n",
        "    total_eval_loss = 0\n",
        "    #nb_eval_steps = 0\n",
        "\n",
        "    y_true = []\n",
        "    y_pred = []\n",
        "\n",
        "    # Evaluate data for one epoch\n",
        "    for batch in valid_dataloader:\n",
        "        \n",
        "        # Unpack this training batch from our dataloader. \n",
        "        #\n",
        "        # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "        # the `to` method.\n",
        "        #\n",
        "        # `batch` contains three pytorch tensors:\n",
        "        #   [0]: input ids \n",
        "        #   [1]: attention masks\n",
        "        #   [2]: labels \n",
        "        b_input_ids = batch[0].to(device)\n",
        "        b_input_mask = batch[1].to(device)\n",
        "        b_labels = batch[2].to(device)\n",
        "        \n",
        "        # Tell pytorch not to bother with constructing the compute graph during\n",
        "        # the forward pass, since this is only needed for backprop (training).\n",
        "        with torch.no_grad():        \n",
        "\n",
        "            # Forward pass, calculate logit predictions.\n",
        "            # Get the \"logits\" output by the model. The \"logits\" are the output\n",
        "            # values prior to applying an activation function like the softmax.\n",
        "            (loss, logits) = model(b_input_ids, \n",
        "                                   token_type_ids=None, \n",
        "                                   attention_mask=b_input_mask,\n",
        "                                   labels=b_labels)[:2]\n",
        "            #loss, logits =  outputs[:2]\n",
        "            \n",
        "        # Accumulate the validation loss.\n",
        "        total_eval_loss += loss.item()\n",
        "\n",
        "        # Move logits and labels to CPU\n",
        "        logits = torch.argmax(F.log_softmax(logits,dim=2),dim=2)\n",
        "        logits = logits.detach().cpu().numpy()\n",
        "        label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "         # Only predict the real word, mark=0, will not calculate\n",
        "        input_mask = b_input_mask.to('cpu').numpy()\n",
        "\n",
        "        # Transform logits into Labels and compare them for each sentence\n",
        "        for i,mask in enumerate(input_mask):\n",
        "          # Real one\n",
        "          temp_1 = []\n",
        "          # Predict one\n",
        "          temp_2 = []\n",
        "          ## check Attention mask\n",
        "          for j, m in enumerate(mask):\n",
        "            #  print(j)\n",
        "            #  print(m)\n",
        "          # Mark=0, meaning its a pad word, dont compare\n",
        "              if m:\n",
        "                  ### Checking [CLS] and [SEP] Tokens\n",
        "                  if tag2name[label_ids[i][j]] != \"None\" :\n",
        "                      temp_1.append(tag2name[label_ids[i][j]])\n",
        "                      temp_2.append(tag2name[logits[i][j]])\n",
        "              else:\n",
        "                  break\n",
        "        \n",
        "            \n",
        "          y_true.append(temp_1)\n",
        "          y_pred.append(temp_2)\n",
        "\n",
        "        \n",
        "        \n",
        "    y_true_words = [word if word!='O' else word for require in y_true for word in require ]\n",
        "    y_pred_words = [word if word!='O' else word for require in y_pred for word in require ]\n",
        "    \n",
        "    scores = precision_recall_fscore_support(y_true_words,y_pred_words,labels = [label for label in set(y_true_words)if label!='O'])[2:]\n",
        "    ## result dicts \n",
        "    f1_scores = {}\n",
        "    examples = {}\n",
        "    ## add values for each Label\n",
        "    names =  [label for label in set(y_true_words)if label!='O']\n",
        "    for i, label in enumerate(names):\n",
        "\n",
        "      f1_scores[label] = scores[0][i]\n",
        "      examples[label]= scores[1][i]   \n",
        "    \n",
        "    ## adds averaged scores and summed up examples to result dic\n",
        "    f1_scores['weighted'] = f1_score(y_true_words, y_pred_words, average='weighted',labels =[label for label in set(y_true_words)if label!='O'])\n",
        "    examples['sum']= np.sum([examples[key] for key in examples.keys()])\n",
        "    # Calculate the average loss over all of the batches.\n",
        "    avg_val_loss = total_eval_loss / len(valid_dataloader)\n",
        "\n",
        "    ## print validation loss and F1 score for epoch \n",
        "    \n",
        "    print(\"  F1_score: {0:.2f}\".format(f1_scores['weighted']))\n",
        "    print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "\n",
        "    \n",
        "    # Measure how long the validation run took.\n",
        "    validation_time = format_time(time.time() - t0)\n",
        "    \n",
        "    print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "    # Record all statistics from this epoch.\n",
        "    training_stats.append(\n",
        "        {\n",
        "            'epoch': epoch_i + 1,\n",
        "            'Training Loss': avg_train_loss,\n",
        "            'Valid. Loss': avg_val_loss,\n",
        "            'F1 score': f1_scores['weighted'],\n",
        "            'examples_sum': examples['sum'],\n",
        "            'Label_F1_scores':f1_scores,\n",
        "            'examples'    : examples,\n",
        "            'Training Time': training_time,\n",
        "            'Validation Time': validation_time\n",
        "        }\n",
        "    )\n",
        "    ## save training values to file\n",
        "    with open(\"Train_results.json\", 'w+', encoding='utf-8') as file:\n",
        "                      pd.DataFrame(training_stats).to_json(file, orient='records', force_ascii=False)\n",
        "\n",
        "print(\"\")\n",
        "print(\"Training complete!\")\n",
        "\n",
        "print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.12\n",
            "  Training epoch took: 0:00:36\n",
            "\n",
            "Running Validation...\n",
            "  F1_score: 0.69\n",
            "  Validation Loss: 0.57\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.07\n",
            "  Training epoch took: 0:00:36\n",
            "\n",
            "Running Validation...\n",
            "  F1_score: 0.70\n",
            "  Validation Loss: 0.57\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epoch took: 0:00:36\n",
            "\n",
            "Running Validation...\n",
            "  F1_score: 0.71\n",
            "  Validation Loss: 0.57\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epoch took: 0:00:36\n",
            "\n",
            "Running Validation...\n",
            "  F1_score: 0.71\n",
            "  Validation Loss: 0.58\n",
            "  Validation took: 0:00:03\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:02:38 (h:mm:ss)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BVFcm8TaQwdm",
        "outputId": "68b4427b-b9c8-4578-c787-7394a66e1c17"
      },
      "source": [
        "## Print classification report with results \n",
        "y_true_words = [word if word!='O' else word for require in y_true for word in require ]\n",
        "y_pred_words = [word if word!='O' else word for require in y_pred for word in require ]\n",
        "report =classification_report(y_true_words, y_pred_words, digits=3, labels = [label for label in set(y_true_words)if label!='O'])\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                                      precision    recall  f1-score   support\n",
            "\n",
            "               Structure & Mechanism      0.477     0.614     0.537        83\n",
            "                             Thermal      0.626     0.898     0.738       108\n",
            "                   Space Environment      0.763     0.758     0.760       293\n",
            "                                GN&C      0.594     0.656     0.624        96\n",
            "                            Telecom.      0.549     0.626     0.585        99\n",
            "                         Cleanliness      0.893     0.633     0.741        79\n",
            "                          Propulsion      0.603     0.407     0.486        86\n",
            "                       Nonconformity      0.500     0.426     0.460        47\n",
            "                                OBDH      0.831     0.744     0.785       238\n",
            "                         Measurement      0.881     0.922     0.901       217\n",
            "                           Parameter      0.446     0.438     0.442       160\n",
            "                     Quality control      0.707     0.658     0.682       231\n",
            "             Safety / Risk (Control)      0.773     0.600     0.675        85\n",
            "                               Power      0.792     0.797     0.795       182\n",
            "Project Organisation / Documentation      0.343     0.277     0.307        83\n",
            "                       Project Scope      0.574     0.413     0.481        75\n",
            "                    Materials / EEEs      0.687     0.606     0.644        94\n",
            "                  System engineering      0.639     0.650     0.645       120\n",
            "\n",
            "                           micro avg      0.685     0.667     0.676      2376\n",
            "                           macro avg      0.649     0.618     0.627      2376\n",
            "                        weighted avg      0.688     0.667     0.673      2376\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dWSZ_J0lWOPT"
      },
      "source": [
        "## Save fine-tuned model in drive / to disc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zgx2PmCHXp5k"
      },
      "source": [
        "#### Connect to drive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCiEKwm9OEM4",
        "outputId": "0c57709f-bca3-4a29-90ae-cec398a8ca8a"
      },
      "source": [
        "## connect the notebook to your own Google Drive storage \n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1DvwotywSQDP"
      },
      "source": [
        "#### Save / load model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CFmwduT0D2c5",
        "outputId": "06ac78a3-b0f9-4499-963d-fb622da5debe"
      },
      "source": [
        "## Save Fine-tuned model \n",
        "## Specify path in google drive\n",
        "path = \"/content/drive/My Drive/\"\n",
        "model_out_address = path +'models/Fine-tuned_SpaceBERT'\n",
        "## Save label dicts in model config for loading the models later again \n",
        "model.config.id2label = {key: tag2name[key] for key in tag2name.keys() - {-100}} ## -100 Key need to be deleted from dict otherwise error when loading model again \n",
        "model.config.label2id = tag2idx\n",
        "\n",
        "## Save model\n",
        "model.save_pretrained(model_out_address, save_config = True)\n",
        "## Save tokeniser\n",
        "tokenizer.save_pretrained(model_out_address)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "('/content/drive/My Drive/models/Fine-tuned_SpaceBERT/tokenizer_config.json',\n",
              " '/content/drive/My Drive/models/Fine-tuned_SpaceBERT/special_tokens_map.json',\n",
              " '/content/drive/My Drive/models/Fine-tuned_SpaceBERT/vocab.txt',\n",
              " '/content/drive/My Drive/models/Fine-tuned_SpaceBERT/added_tokens.json',\n",
              " '/content/drive/My Drive/models/Fine-tuned_SpaceBERT/tokenizer.json')"
            ]
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oVe4iQuuF13w"
      },
      "source": [
        "##Load saved model from drive \n",
        "model = AutoModelForTokenClassification.from_pretrained(model_out_address)#num_labels=len(tag2idx))\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_out_address)\n",
        "## Train with GPU, if available\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda();\n",
        "    if n_gpu >1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "f7pVmsEsw1sp"
      },
      "source": [
        "## Or load CR model from Huggingface hub \n",
        "model_file_address = 'icelab/spacescibert_CR'\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"icelab/spacescibert_CR\")\n",
        "\n",
        "model = AutoModelForTokenClassification.from_pretrained(model_file_address)#num_labels=len(tag2idx))\n",
        "n_gpu = torch.cuda.device_count()\n",
        "if torch.cuda.is_available():\n",
        "    model.cuda();\n",
        "    if n_gpu >1:\n",
        "        model = torch.nn.DataParallel(model)\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHvQ7hqpElAl"
      },
      "source": [
        "## Example labeling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4ZMhjZIWLj9N"
      },
      "source": [
        "text =\"The CubeSat RF design shall either have one RF inhibit and a RF power output no greater than 1.5W at the transmitter antenna's RF input OR the CubeSat shall have a minimum of two independent RF inhibits (CDS 3.3.9) (ISO 5.5.6). \""
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hvAl5p_VLbL-"
      },
      "source": [
        "import numpy as np\n",
        "import spacy\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "#model.eval();\n",
        "sentences = []\n",
        "#spacy.require_gpu()\n",
        "docs = nlp.tokenizer(text)\n",
        "#docs = list(docs)\n",
        "#for sentence in docs:\n",
        "sentences = [word.text for word in docs]\n",
        "\n",
        "encoded_input = tokenizer(sentences, return_tensors=\"pt\", padding=True, is_split_into_words=True)#, truncation=True, max_length=512)\n",
        "input_ids = encoded_input['input_ids']\n",
        "attention_masks = encoded_input[\"attention_mask\"]\n",
        "\n",
        "#b_input_ids = batch[0].to(device)\n",
        "#b_input_mask = batch[1].to(device)\n",
        "i_ids = input_ids.to(device)\n",
        "a_masks = attention_masks.to(device)\n",
        "with torch.no_grad():  \n",
        "  prediction = model(i_ids, token_type_ids=None, attention_mask=a_masks)[0]\n",
        "\n",
        "logits = torch.argmax(F.log_softmax(prediction, dim=2), dim=2)\n",
        "logits = logits.detach().cpu().numpy()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wVRO232bBHSy"
      },
      "source": [
        "def predict_spans(text, model): \n",
        "    \n",
        "    \"\"\" \n",
        "    Function to take a text as input and output span object for use with spacy\n",
        "    text = str\n",
        "    model = Huggingface.Tokenclassificationmodel\n",
        "    \"\"\"\n",
        "\n",
        "    nlp = spacy.load(\"en_core_web_sm\")\n",
        "    model.eval();\n",
        "    sentences = []\n",
        "\n",
        "    docs = nlp.tokenizer(text)\n",
        "\n",
        "    ## use spacy to tokenize the requirement \n",
        "\n",
        "    word_list = [word.text for word in docs]\n",
        "\n",
        "    ## tokenize the word with loaded tokenizer\n",
        "\n",
        "    encoded_input = tokenizer(word_list, return_tensors=\"pt\", padding=True, is_split_into_words=True, truncation=True, max_length=512)\n",
        "    input_ids = encoded_input['input_ids']\n",
        "    attention_masks = encoded_input[\"attention_mask\"]\n",
        "\n",
        "    ## load arrays on device (GPU)\n",
        "    i_ids = input_ids.to(device)\n",
        "    a_masks = attention_masks.to(device)\n",
        "\n",
        "    with torch.no_grad():  \n",
        "      prediction = model(i_ids, token_type_ids=None, attention_mask=a_masks, )[0]\n",
        "\n",
        "    logits = torch.argmax(F.log_softmax(prediction, dim=2), dim=2)\n",
        "    logits = logits.detach().cpu().numpy()\n",
        "\n",
        "    tokens = []\n",
        "    spans = []\n",
        "    tag2name = model.config.id2label\n",
        "    tag2name[-100]=None \n",
        "    ## Tags from logits without added [CLS] / [SEP] tokens\n",
        "    tags_s = [tag2name[t] for t in logits[0][1:-1]]\n",
        "    #scores = scores[1:-1]\n",
        "    # Count if word was split by tokenizer to split labels of prediction\n",
        "    j = 0\n",
        "    ## Tags, adjusted to wordpiece tokenisation\n",
        "    tags_r = []\n",
        "    #scores_r = []\n",
        "    for word_count, word in enumerate(doc):\n",
        "        ## Tokenise each word of the sentence\n",
        "        word_ids = tokenizer.tokenize(word.text)\n",
        "\n",
        "        ## Tokeniser of Spacy tokenises \"/n\" --> Tokenizer of BERT & RoBERTa doesn't --> len(tokenizer.tokenize(\"\\n\"))= 0 --> Have to tokenise accordingly\n",
        "        if len(word_ids) == 0:\n",
        "          j -= 1\n",
        "          tags_r.append('O')\n",
        "          #scores_r.append(0)\n",
        "          pass\n",
        "        ## == 1 --> Words get not split by word-piece tokeniser\n",
        "        elif len(word_ids) == 1:\n",
        "          # spans.append(mappings[word_count])#s[word_count])\n",
        "          tags_r.append(tags_s[word_count + j])\n",
        "        # scores_r.append(scores[word_count + j])\n",
        "          # pass\n",
        "        ## Word gets split --> Only one prediction gets added to list of tags\n",
        "        else:\n",
        "          tags_r.append(tags_s[word_count + j])\n",
        "        # scores_r.append(scores[word_count + j])\n",
        "          j += (len(word_ids) - 1)\n",
        "\n",
        "\n",
        "    #### Get spans in input sentence\n",
        "                \n",
        "    ## Label of previous word\n",
        "    temp_label = ''\n",
        "    ## word counter, how many steps to go back for complete sequence\n",
        "    j = 0\n",
        "    for word_count, word in enumerate(doc):\n",
        "        ## Tag is \"0\" and no Label in previous word\n",
        "        if tags_r[word_count] == 'O' and j == 0:\n",
        "            pass\n",
        "        ## Tag is same as before\n",
        "        elif temp_label == tags_r[word_count][2:]:\n",
        "            j -= 1\n",
        "        ## Tag is \"0\" indicating that concept is complete --> add to span and reset word_counter\n",
        "        elif tags_r[word_count] == 'O' and j != 0:\n",
        "            spans.append({\"start\": doc[word_count + j].idx,\n",
        "                          \"end\": (doc[word_count - 1].idx + len(doc[word_count - 1].text)),\n",
        "                          'token_start': doc[word_count + j].i,\n",
        "                          'token_end': doc[word_count - 1].i, \"label\": tags_r[word_count - 1], \n",
        "                      #   'score': np.mean(scores_r[word_count +j:word_count])\n",
        "                          })# [2:]\n",
        "            j = 0  #\n",
        "            temp_label = ''\n",
        "        ## Tag is a Label, word counter starts\n",
        "        else:\n",
        "            temp_label = tags_r[word_count][2:]\n",
        "            j -= 1\n",
        "    return spans"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XlmiJFKr-Fm_",
        "outputId": "8f672ed0-8e77-4033-99a8-d3d33de618f4"
      },
      "source": [
        "nlp = spacy.load('en_core_web_sm', disable=['ner'])\n",
        "doc = nlp(text)\n",
        "spans = predict_spans(text, model)\n",
        "concepts =[]\n",
        "for annot in spans:\n",
        "                #concepts.append({'text': data[sentence]['text'],'word': doc.char_span(data[sentence]['spans'][annot]['start'], data[sentence]['spans'][annot]['end'], label=data[sentence]['spans'][annot]['label']).text, 'label':data[sentence]['spans'][annot]['label']})\n",
        "                concepts.append({'word': doc.char_span(annot['start'], annot['end'], label=annot['label']).text, 'label':annot['label']#, 'score':annot['score']\n",
        "                                 })\n",
        "concepts\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'label': 'Telecom.', 'word': 'CubeSat RF design'},\n",
              " {'label': 'Telecom.', 'word': 'RF inhibit'},\n",
              " {'label': 'Telecom.', 'word': 'RF power output'},\n",
              " {'label': 'Measurement', 'word': '1.5W'},\n",
              " {'label': 'Telecom.', 'word': 'transmitter antenna'},\n",
              " {'label': 'Telecom.', 'word': 'RF input'},\n",
              " {'label': 'System engineering', 'word': 'CubeSat'},\n",
              " {'label': 'Telecom.', 'word': 'RF inhibits'}]"
            ]
          },
          "metadata": {},
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rsj5OeICD196"
      },
      "source": [
        "Visualise with spaCy"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6MAaUTwZ-JYl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 87
        },
        "outputId": "9ccfc502-1a5c-4b4d-d478-e22a2828e37f"
      },
      "source": [
        "from spacy.tokens import Span\n",
        "from spacy import displacy\n",
        "## https://spacy.io/usage/visualizers\n",
        "for span in spans:\n",
        "    doc.ents = list(doc.ents) + [Span(doc, span['token_start'], span['token_end']+1, span['label'])]\n",
        "\n",
        "spacy.displacy.render(doc, style='ent', jupyter=True, options={'distance': 90})"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<span class=\"tex2jax_ignore\"><div class=\"entities\" style=\"line-height: 2.5; direction: ltr\">The \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    CubeSat RF design\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n",
              "</mark>\n",
              " shall either have one \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    RF inhibit\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n",
              "</mark>\n",
              " and a \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    RF power output\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n",
              "</mark>\n",
              " no greater than \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    1.5W\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Measurement</span>\n",
              "</mark>\n",
              " at the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    transmitter antenna\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n",
              "</mark>\n",
              "'s \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    RF input\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n",
              "</mark>\n",
              " OR the \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    CubeSat\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">System engineering</span>\n",
              "</mark>\n",
              " shall have a minimum of two independent \n",
              "<mark class=\"entity\" style=\"background: #ddd; padding: 0.45em 0.6em; margin: 0 0.25em; line-height: 1; border-radius: 0.35em;\">\n",
              "    RF inhibits\n",
              "    <span style=\"font-size: 0.8em; font-weight: bold; line-height: 1; border-radius: 0.35em; text-transform: uppercase; vertical-align: middle; margin-left: 0.5rem\">Telecom.</span>\n",
              "</mark>\n",
              " (CDS 3.3.9) (ISO 5.5.6). </div></span>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4czpd3zYwpQt"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
{"status": 200, "content": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nDigital signal processor - Wikipedia\n\n\n\n\n\n\n\n\t\n\t\n\n\t\n\n\n\n\tDigital signal processor\n\n\t\n\t\n\t\tFrom Wikipedia, the free encyclopedia\n\n\t\t\n\n\t\t\n\t\t\n\t\t\n\t\t\n\n\t\tJump to navigation\n\t\tJump to search\n\t\t  \nA digital signal processor chip found in a guitar effects unit.\n\n\n\nA digital signal processor (DSP) is a specialized microprocessor (or a SIP block), with its architecture optimized for the operational needs of digital signal processing.[1][2]\n\nThe goal of DSP is usually to measure, filter or compress continuous real-world analog signals.  Most general-purpose microprocessors can also execute digital signal processing algorithms successfully, but may not be able to keep up with such processing continuously in real-time.  Also, dedicated DSPs usually have better power efficiency, thus they are more suitable in portable devices such as mobile phones because of power consumption constraints.[3]  DSPs often use special memory architectures that are able to fetch multiple data or instructions at the same time.\n\n\nContents\n\n\n\t1 Overview\n\t2 Architecture\n\t2.1 Software architecture\n\t2.1.1 Instruction sets\n\t2.1.2 Data instructions\n\t2.1.3 Program flow\n\n\n\n\t2.2 Hardware architecture\n\t2.2.1 Memory architecture\n\t2.2.1.1 Addressing and virtual memory\n\n\n\n\n\n\n\n\n\n\t3 History\n\t4 Modern DSPs\n\t5 See also\n\t6 References\n\t7 External links\n\n\n\n\n\nOverview[edit]\n\n  \nA typical digital processing system\n\n\n\nDigital signal processing algorithms typically require a large number of mathematical operations to be performed quickly and repeatedly on a series of data samples.  Signals (perhaps from audio or video sensors) are constantly converted from analog to digital, manipulated digitally, and then converted back to analog form. Many DSP applications have constraints on latency; that is, for the system to work, the DSP operation must be completed within some fixed time, and deferred (or batch) processing is not viable.\n\nMost general-purpose microprocessors and operating systems can execute DSP algorithms successfully, but are not suitable for use in portable devices such as mobile phones and PDAs because of power efficiency constraints.[3] A specialized DSP, however, will tend to provide a lower-cost solution, with better performance, lower latency, and no requirements for specialised cooling or large batteries.[citation needed]\n\nSuch performance improvements have led to the introduction of digital signal processing in commercial communications satellites where hundreds or even thousands of analog filters, switches, frequency converters and so on are required to receive and process the uplinked signals and ready them for downlinking, and can be replaced with specialised DSPs with a significant benefits to the satellites' weight, power consumption, complexity/cost of construction, reliability and flexibility of operation. For example, the SES-12 and SES-14 satellites from operator  SES, both intended for launch in 2017, were built by Airbus Defence and Space with 25% of capacity using DSP.[4]\n\nThe architecture of a DSP is optimized specifically for digital signal processing. Most also support some of the features as an applications processor or microcontroller, since signal processing is rarely the only task of a system. Some useful features for optimizing DSP algorithms are outlined below.\n\n\nArchitecture[edit]\n\nSoftware architecture[edit]\n\nBy the standards of general-purpose processors, DSP instruction sets are often highly irregular; while traditional instruction sets are made up of more general instructions that allow them to perform a wider variety of operations, instruction sets optimized for digital signal processing contain instructions for common mathematical operations that occur frequently in DSP calculations. Both traditional and DSP-optimized instruction sets are able to compute any arbitrary operation but an operation that might require multiple ARM or x86 instructions to compute might require only one instruction in a DSP optimized instruction set.\n\nOne implication for software architecture is that hand-optimized assembly-code routines are commonly packaged into libraries for re-use, instead of relying on advanced compiler technologies to handle essential algorithms.[clarification needed] Even with modern compiler optimizations hand-optimized assembly code is more efficient and many common algorithms involved in DSP calculations are hand-written in order to take full advantage of the architectural optimizations.\n\n\nInstruction sets[edit]\n\n\tmultiply\u2013accumulates (MACs, including fused multiply\u2013add, FMA) operations\n\tused extensively in all kinds of matrix operations\n\tconvolution for filtering\n\tdot product\n\tpolynomial evaluation\n\n\n\tFundamental DSP algorithms depend heavily on multiply\u2013accumulate performance\n\tFIR filters\n\tFast Fourier transform (FFT)\n\n\n\n\n\tInstructions to increase parallelism:\n\tSIMD\n\tVLIW\n\tsuperscalar architecture\n\n\n\tSpecialized instructions for modulo addressing in ring buffers and bit-reversed addressing mode for FFT cross-referencing\n\tDSPs sometimes use time-stationary encoding to simplify hardware and increase coding efficiency.\n\tMultiple arithmetic units may require memory architectures to support several accesses per instruction cycle\n\tSpecial loop controls, such as architectural support for executing a few instruction words in a very tight loop without overhead for instruction fetches or exit testing[clarification needed]\n\n\nData instructions[edit]\n\n\tSaturation arithmetic, in which operations that produce overflows will accumulate at the maximum (or minimum) values that the register can hold rather than wrapping around (maximum+1 doesn't overflow to minimum as in many general-purpose CPUs, instead it stays at maximum). Sometimes various sticky bits operation modes are available.\n\tFixed-point arithmetic is often used to speed up arithmetic processing\n\tSingle-cycle operations to increase the benefits of pipelining\n\n\nProgram flow[edit]\n\n\tFloating-point unit integrated directly into the datapath\n\tPipelined architecture\n\tHighly parallel multiplier\u2013accumulators (MAC units)\n\tHardware-controlled looping, to reduce or eliminate the overhead required for looping operations\n\n\nHardware architecture[edit]\n\nIn engineering, hardware architecture refers to the identification of a system's physical components and their interrelationships. This description, often called a hardware design model, allows hardware designers to understand how their components fit into a system architecture and provides to software component designers important information needed for software development and integration. Clear definition of a hardware architecture allows the various traditional engineering disciplines (e.g., electrical and mechanical engineering) to work more effectively together to develop and manufacture new machines, devices and components.\n\nHardware is also an expression used within the computer engineering industry to explicitly distinguish the (electronic computer) hardware from the software that runs on it. But hardware, within the automation and software engineering disciplines, need not simply be a computer of some sort. A modern automobile runs vastly more software than the Apollo spacecraft. Also, modern aircraft cannot function without running tens of millions of computer instructions embedded and distributed throughout the aircraft and resident in both standard computer hardware and in specialized hardware components such as IC wired logic gates, analog and hybrid devices, and other digital components. The need to effectively model how separate physical components combine to form complex systems is important over a wide range of applications, including computers, personal digital assistants (PDAs), cell phones, surgical instrumentation, satellites, and submarines.\n\n\nMemory architecture[edit]\n\nDSPs are usually optimized for streaming data and use special memory architectures that are able to fetch multiple data or instructions at the same time, such as the Harvard architecture or Modified von Neumann architecture, which use separate program and data memories (sometimes even concurrent access on multiple data buses).\n\nDSPs can sometimes rely on supporting code to know about cache hierarchies and the associated delays.  This is a tradeoff that allows for better performance[clarification needed].  In addition, extensive use of DMA is employed.\n\n\nAddressing and virtual memory[edit]\n\nDSPs frequently use multi-tasking operating systems, but have no support for virtual memory or memory protection.  Operating systems that use virtual memory require more time for context switching among processes, which increases latency.\n\n\n\tHardware modulo addressing\n\tAllows circular buffers to be implemented without having to test for wrapping\n\n\n\tBit-reversed addressing, a special addressing mode\n\tuseful for calculating FFTs\n\n\n\tExclusion of a memory management unit\n\tAddress generation unit\n\n\nHistory[edit]\n\nPrior to the advent of stand-alone DSP chips discussed below, most DSP applications were implemented using bit-slice processors.  The AMD 2901 bit-slice chip with its family of components was a very popular choice. There were reference designs from AMD, but very often the specifics of a particular design were application specific. These bit slice architectures would sometimes include a peripheral multiplier chip. Examples of these multipliers were a series from TRW including the TDC1008 and TDC1010, some of which included an accumulator, providing the requisite multiply\u2013accumulate (MAC) function.\n\nIn 1976, Richard Wiggins proposed the Speak & Spell concept to Paul Breedlove, Larry Brantingham, and Gene Frantz at Texas Instrument's Dallas research facility.  Two years later in 1978 they produced the first Speak & Spell, with the technological centerpiece being the TMS5100,[5] the industry's first digital signal processor.  It also set other milestones, being the first chip to use Linear predictive coding to perform speech synthesis.[6]\n\nIn 1978, Intel released the 2920 as an \"analog signal processor\".[7] It had an on-chip ADC/DAC with an internal signal processor, but it didn't have a hardware multiplier and was not successful in the market. In 1979, AMI released the S2811.\nThe AMI S2811 \"signal processing peripheral\", like many later DSPs, has a hardware multiplier that enables it to do multiply\u2013accumulate operation in a single instruction.[8]\nIt was designed as a microprocessor peripheral, and it had to be initialized by the host. The S2811 was likewise not successful in the market.\n\nIn 1980 the first stand-alone, complete DSPs \u2013 the NEC \u00b5PD7720 and AT&T DSP1 \u2013 were presented at the International Solid-State Circuits Conference '80. Both processors were inspired by the research in PSTN telecommunications.\n\nThe Altamira DX-1 was another early DSP, utilizing quad integer pipelines with delayed branches and branch prediction.[citation needed]\n\nAnother DSP produced by Texas Instruments (TI), the TMS32010 presented in 1983, proved to be an even bigger success. It was based on the Harvard architecture, and so had separate instruction and data memory. It already had a special instruction set, with instructions like load-and-accumulate or multiply-and-accumulate. It could work on 16-bit numbers and needed 390\u00a0ns for a multiply\u2013add operation. TI is now the market leader in general-purpose DSPs.\n\nAbout five years later, the second generation of DSPs began to spread. They had 3 memories for storing two operands simultaneously and included hardware to accelerate tight loops; they also had an addressing unit capable of loop-addressing. Some of them operated on 24-bit variables and a typical model only required about 21\u00a0ns for a MAC. Members of this generation were for example the AT&T DSP16A or the Motorola 56000.\n\nThe main improvement in the third generation was the appearance of application-specific units and instructions in the data path, or sometimes as coprocessors. These units allowed direct hardware acceleration of very specific but complex mathematical problems, like the Fourier-transform or matrix operations. Some chips, like the Motorola MC68356, even included more than one processor core to work in parallel. Other DSPs from 1995 are the TI TMS320C541 or the TMS 320C80.\n\nThe fourth generation is best characterized by the changes in the instruction set and the instruction encoding/decoding. SIMD extensions were added, and VLIW and the superscalar architecture appeared. As always, the clock-speeds have increased; a 3\u00a0ns MAC now became possible.\n\n\nModern DSPs[edit]\n\nModern signal processors yield greater performance; this is due in part to both technological and architectural advancements like lower design rules, fast-access two-level cache, (E)DMA circuitry and a wider bus system. Not all DSPs provide the same speed and many kinds of signal processors exist, each one of them being better suited for a specific task, ranging in price from about US$1.50 to US$300.\n\nTexas Instruments produces the C6000 series DSPs, which have clock speeds of 1.2\u00a0GHz and implement separate instruction and data caches. They also have an 8 MiB 2nd level cache and 64 EDMA channels. The top models are capable of as many as 8000 MIPS (millions of instructions per second), use VLIW (very long instruction word), perform eight operations per clock-cycle and are compatible with a broad range of external peripherals and various buses (PCI/serial/etc). TMS320C6474 chips each have three such DSPs, and the newest generation C6000 chips support floating point as well as fixed point processing.\n\nFreescale produces a multi-core DSP family, the MSC81xx. The MSC81xx is based on StarCore Architecture processors and the latest MSC8144 DSP combines four programmable SC3400 StarCore DSP cores. Each SC3400 StarCore DSP core has a clock speed of 1\u00a0GHz.\n\nXMOS produces a multi-core multi-threaded line of processor well suited to DSP operations, They come in various speeds ranging from 400 to 1600 MIPS. The processors have a multi-threaded architecture that allows up to 8 real-time threads per core, meaning that a 4 core device would support up to 32 real time threads. Threads communicate between each other with buffered channels that are capable of up to 80 Mbit/s. The devices are easily programmable in C and aim at bridging the gap between conventional micro-controllers and FPGAs\n\nCEVA, Inc. produces and licenses three distinct families of DSPs. Perhaps the best known and most widely deployed is the CEVA-TeakLite DSP family, a classic memory-based architecture, with 16-bit or 32-bit word-widths and single or dual MACs. The CEVA-X DSP family offers a combination of VLIW and SIMD architectures, with different members of the family offering dual or quad 16-bit MACs. The CEVA-XC DSP family targets Software-defined Radio (SDR) modem designs and leverages a unique combination of VLIW and Vector architectures with 32 16-bit MACs.\n\nAnalog Devices produce the SHARC-based DSP and range in performance from 66\u00a0MHz/198 MFLOPS (million floating-point operations per second) to 400\u00a0MHz/2400 MFLOPS. Some models support multiple multipliers and ALUs, SIMD instructions and audio processing-specific components and peripherals. The Blackfin family of embedded digital signal processors combine the features of a DSP with those of a general use processor. As a result, these processors can run simple operating systems like \u03bcCLinux, velocity and Nucleus RTOS while operating on real-time data.\n\nNXP Semiconductors produce DSPs based on TriMedia VLIW technology, optimized for audio and video processing. In some products the DSP core is hidden as a fixed-function block into a SoC, but NXP also provides a range of flexible single core media processors. The TriMedia media processors support both fixed-point arithmetic as well as floating-point arithmetic, and have specific instructions to deal with complex filters and entropy coding.\n\nCSR produces the Quatro family of SoCs that contain one or more custom Imaging DSPs optimized for processing document image data for scanner and copier applications.\n\nMicrochip Technology produces the PIC24 based dsPIC line of DSPs. Introduced in 2004, the dsPIC is designed for applications needing a true DSP as well as a true microcontroller, such as motor control and in power supplies. The dsPIC runs at up to 40MIPS, and has support for 16 bit fixed point MAC, bit reverse and modulo addressing, as well as DMA.\n\nMost DSPs use fixed-point arithmetic, because in real world signal processing the additional range provided by floating point is not needed, and there is a large speed benefit and cost benefit due to reduced hardware complexity. Floating point DSPs may be invaluable in applications where a wide dynamic range is required. Product developers might also use floating point DSPs to reduce the cost and complexity of software development in exchange for more expensive hardware, since it is generally easier to implement algorithms in floating point.\n\nGenerally, DSPs are dedicated integrated circuits; however DSP functionality can also be produced by using field-programmable gate array chips (FPGAs).\n\nEmbedded general-purpose RISC processors are becoming increasingly DSP like in functionality. For example, the OMAP3 processors include a ARM Cortex-A8 and C6000 DSP.\n\nIn Communications a new breed of DSPs offering the fusion of both DSP functions and H/W acceleration function is making its way into the mainstream. Such Modem processors include ASOCS ModemX and CEVA's XC4000.\n\nIn May 2018, Huarui-2 designed by Nanjing Research Institute of Electronics Technology passed acceptance. With a processing speed of 0.4 TFLOPS, the chip can achieve better performance than current mainstream DSP chips.[9] The design team has begun to create Huarui-3, which has a processing speed in TFLOPS level and a support for artificial intelligence.[10]\n\n\nSee also[edit]\n\n\tDigital signal controller\n\tGraphics processing unit\n\tSystem on a chip\n\tHardware acceleration\n\tVision processing unit\n\tMDSP \u2013 a multiprocessor DSP\n\tOpenCL\n\n\nReferences[edit]\n\n\n\t^ Dyer, S. A.; Harms, B. K. (1993). \"Digital Signal Processing\".  In Yovits, M. C. (ed.). Advances in Computers. 37. Academic Press. pp.\u00a0104\u2013107. doi:10.1016/S0065-2458(08)60403-9. ISBN\u00a09780120121373.\n\n\t^ Liptak, B. G. (2006). Process Control and Optimization. Instrument Engineers' Handbook. 2 (4th ed.). CRC Press. pp.\u00a011\u201312. ISBN\u00a09780849310812.\n\n\n\t^ a b Ingrid Verbauwhede; Patrick Schaumont; Christian Piguet; Bart Kienhuis (2005-12-24). \"Architectures and Design techniques for energy efficient embedded DSP and multimedia processing\" (PDF). rijndael.ece.vt.edu. Retrieved 2017-06-13.\n\n\n\t^ Beyond Frontiers Broadgate Publications (September 2016) pp22\n\n\t^ \"Speak & Spell, the First Use of a Digital Signal Processing IC for Speech Generation, 1978\". IEEE Milestones. IEEE. Retrieved 2012-03-02.\n\n\n\t^ Bogdanowicz, A. (2009-10-06). \"IEEE Milestones Honor Three\". The Institute. IEEE. Archived from the original on 2016-03-04. Retrieved 2012-03-02.\n\n\n\t^ https://www.intel.com/Assets/PDF/General/35yrs.pdf#page=17\n\n\t^ \nAlberto Luis Andres.\n\"Digital Graphic Audio Equalizer\".\np. 48.\n\n\t^ \"\u56fd\u4ea7\u65b0\u578b\u96f7\u8fbe\u82af\u7247\u534e\u777f2\u53f7\u4e0e\u7ec4\u7f51\u4e2d\u5fc3\u540c\u65f6\u4eae\u76f8-\u79d1\u6280\u65b0\u95fb-\u4e2d\u56fd\u79d1\u6280\u7f51\u9996\u9875\". \u79d1\u6280\u65e5\u62a5. Retrieved 2 July 2018.\n\n\n\t^ \u738b\u73cf\u73a2. \"\u5168\u56fd\u4ea7\u82af\u7247\u534e\u777f\uff12\u53f7\u901a\u8fc7\"\u6838\u9ad8\u57fa\"\u9a8c\u6536-\u65b0\u534e\u7f51\". Xinhua News Agency. \u5357\u4eac. Retrieved 2 July 2018.\n\n\n\n\n\nExternal links[edit]\n\n\tDSP Online Book\n\tPocket Guide to Processors for DSP - Berkeley Design Technology, INC\n\n\n\t\tv\n\tt\n\te\n\n\nProcessor technologies\n\n\tModels\t\n\tTuring machine\n\tUniversal\n\tPost\u2013Turing\n\tQuantum\n\n\n\tBelt machine\n\tStack machine\n\tFinite-state machine\n\twith datapath\n\tHierarchical\n\tQueue automaton\n\n\n\tRegister machines\n\tCounter\n\tPointer\n\tRandom-access\n\tRandom-access stored program\n\n\n\n\n\n\n\tArchitecture\t\n\tVon Neumann\n\tHarvard\n\tmodified\n\n\n\tDataflow\n\tTransport-triggered\n\tCellular\n\tEndianness\n\tMemory access\n\tNUMA\n\tHUMA\n\tLoad/store\n\tRegister/memory\n\n\n\tCache hierarchy\n\tMemory hierarchy\n\tVirtual memory\n\tSecondary storage\n\n\n\tHeterogeneous\n\tFabric\n\tMultiprocessing\n\tCognitive\n\tNeuromorphic\n\n\n\n\n\tInstruction set\narchitectures\t\n\tTypes\t\n\tCISC\n\tRISC\n\tApplication-specific\n\tEDGE\n\tTRIPS\n\n\n\tVLIW\n\tEPIC\n\n\n\tMISC\n\tOISC\n\tNISC\n\tZISC\n\tcomparison\n\taddressing modes\n\n\n\n\n\n\n\n\n\tx86\n\tARM\n\tMIPS\n\tPower ISA\n\tSPARC\n\tItanium\n\tUnicore\n\tMicroBlaze\n\tRISC-V\n\tothers\n\n\n\n\tExecution\t\n\tInstruction pipelining\t\n\tPipeline stall\n\tOperand forwarding\n\tClassic RISC pipeline\n\n\n\n\tHazards\t\n\tData dependency\n\tStructural\n\tControl\n\tFalse sharing\n\n\n\n\n\tOut-of-order\t\n\tTomasulo algorithm\n\tReservation station\n\tRe-order buffer\n\n\n\tRegister renaming\n\n\n\n\n\tSpeculative\t\n\tBranch prediction\n\tMemory dependence prediction\n\n\n\n\n\n\n\n\tParallelism\t\n\tLevel\t\n\tBit\n\tBit-serial\n\tWord\n\n\n\tInstruction\n\tPipelining\n\tScalar\n\tSuperscalar\n\n\n\tTask\n\tThread\n\tProcess\n\n\n\tData\n\tVector\n\n\n\tMemory\n\tDistributed\n\n\n\n\n\tMultithreading\t\n\tTemporal\n\tSimultaneous\n\tHyperthreading\n\n\n\tSpeculative\n\tPreemptive\n\tCooperative\n\n\n\n\n\tFlynn's taxonomy\t\n\tSISD\n\tSIMD\n\tSWAR\n\n\n\tSIMT\n\tMISD\n\tMIMD\n\tSPMD\n\n\n\n\n\n\n\n\n\n\tProcessor\nperformance\t\n\tTransistor count\n\tInstructions per cycle (IPC)\n\tCycles per instruction (CPI)\n\n\n\tInstructions per second (IPS)\n\tFloating-point operations per second (FLOPS)\n\tTransactions per second (TPS)\n\tSynaptic updates per second (SUPS)\n\tPerformance per watt (PPW)\n\tCache performance metrics\n\tComputer performance by orders of magnitude\n\n\n\n\n\tTypes\t\n\tCentral processing unit (CPU)\n\tGraphics processing unit (GPU)\n\tGPGPU\n\n\n\tVector\n\tBarrel\n\tStream\n\tCoprocessor\n\tASIC\n\tFPGA\n\tCPLD\n\tMulti-chip module (MCM)\n\tSystem in package (SiP)\n\n\n\n\tBy application\t\n\tMicroprocessor\n\tMicrocontroller\n\tMobile\n\tNotebook\n\tUltra-low-voltage\n\tASIP\n\n\n\n\n\tSystems\non chip\t\n\tSystem on a chip (SoC)\n\tMultiprocessor (MPSoC)\n\tProgrammable (PSoC)\n\tNetwork on a chip (NoC)\n\n\n\n\n\tHardware\naccelerators\t\n\tAI accelerator\n\tVision processing unit (VPU)\n\tPhysics processing unit (PPU)\n\tDigital signal processor (DSP)\n\tTensor processing unit (TPU)\n\tSecure cryptoprocessor\n\tNetwork processor\n\tBaseband processor\n\n\n\n\n\n\n\n\n\tWord size\t\n\t1-bit\n\t2-bit\n\t4-bit\n\t8-bit\n\t16-bit\n\t32-bit\n\t48-bit\n\t64-bit\n\t128-bit\n\t256-bit\n\t512-bit\n\tothers\n\tvariable\n\n\n\n\n\n\n\tCore count\t\n\tSingle-core\n\tMulti-core\n\tManycore\n\tHeterogeneous architecture\n\n\n\n\n\tComponents\t\n\tCore\n\tCache\n\tCPU cache\n\treplacement policies\n\tcoherence\n\n\n\tBus\n\tClock rate\n\tClock signal\n\tFIFO\n\n\n\n\tFunctional units\t\n\tArithmetic logic unit (ALU)\n\tAddress generation unit (AGU)\n\tFloating-point unit (FPU)\n\tMemory management unit (MMU)\n\tLoad\u2013store unit\n\tTranslation lookaside buffer (TLB)\n\n\n\tIntegrated memory controller (IMC)\n\n\n\n\n\tLogic\t\n\tCombinational\n\tSequential\n\tGlue\n\tLogic gate\n\tQuantum\n\tArray\n\n\n\n\n\n\n\tRegisters\t\n\tProcessor register\n\tStatus register\n\tStack register\n\tRegister file\n\tMemory buffer\n\tProgram counter\n\n\n\n\n\tControl unit\t\n\tInstruction unit\n\tData buffer\n\tWrite buffer\n\tMicrocode ROM\n\tCounter\n\n\n\n\n\tDatapath\t\n\tMultiplexer\n\tDemultiplexer\n\tAdder\n\tMultiplier\n\tCPU\n\n\n\tBinary decoder\n\tAddress decoder\n\tSum addressed decoder\n\n\n\tBarrel shifter\n\n\n\n\n\tCircuitry\t\n\tIntegrated circuit\n\t3D\n\tMixed-signal\n\tPower management\n\n\n\tBoolean\n\tDigital\n\tAnalog\n\tQuantum\n\tSwitch\n\n\n\n\n\n\n\n\n\tPower\nmanagement\t\n\tPMU\n\tAPM\n\tACPI\n\tDynamic frequency scaling\n\tDynamic voltage scaling\n\tClock gating\n\tPerformance per watt (PPW)\n\n\n\n\n\tRelated\t\n\tHistory of general-purpose CPUs\n\tMicroprocessor chronology\n\tProcessor design\n\tDigital electronics\n\tHardware security module\n\tSemiconductor device fabrication\n\n\n\n\n\n\n\n\tAuthority control \t\n\tGND: 4054943-4\n\n\n\n\n\n\n\n\t\tv\n\tt\n\te\n\n\nHardware acceleration\n\n\tTheory\t\n\tUniversal Turing machine\n\tParallel computing\n\tDistributed computing\n\n\n\n\n\tApplications\t\n\tGPU\n\tGPGPU\n\tDirectX\n\n\n\tAudio\n\tDigital signal processing\n\tHardware random number generation\n\tArtificial intelligence\n\tCryptography\n\tTLS\n\n\n\tMachine vision\n\tCustom hardware attack\n\tscrypt\n\n\n\tNetworking\n\n\n\n\n\tImplementations\t\n\tHigh-level synthesis\n\tC to HDL\n\n\n\tFPGA\n\tASIC\n\tCPLD\n\tSystem on Chip\n\tNetwork on Chip\n\n\n\n\n\n\n\tArchitectures\t\n\tData flow\n\tTransport triggered\n\tMulticore\n\tManycore\n\tHeterogeneous\n\tIn-memory computing\n\tSystolic array\n\tNeuromorphic\n\n\n\n\n\tRelated\t\n\tProgrammable logic\n\tProcessor\n\tdesign\n\tchronology\n\n\n\tDigital electronics\n\tVirtualization\n\tHardware emulation\n\n\n\tLogic synthesis\n\tEmbedded systems\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\t\t\n\t\tRetrieved from \"https://en.wikipedia.org/w/index.php?title=Digital_signal_processor&oldid=916341723\"\n\n\t\t\n\t\tCategories: \tDigital signal processing\n\tDigital signal processors\n\tIntegrated circuits\n\tCoprocessors\n\tHardware acceleration\n\n\nHidden categories: \tAll articles with unsourced statements\n\tArticles with unsourced statements from February 2013\n\tWikipedia articles needing clarification from November 2015\n\tArticles with unsourced statements from December 2014\n\tWikipedia articles with GND identifiers\n\n\n\n\n\t\t\n\n\t\t\n\t\n\n\n\n\n\t\n\n\n\n\n\n\t\t\n\t\t\tNavigation menu\n\n\t\t\t\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tPersonal tools\n\n\t\t\t\t\t\t\tNot logged in\n\tTalk\n\tContributions\n\tCreate account\n\tLog in\n\n\n\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tNamespaces\n\n\t\t\t\t\t\t\tArticle\n\tTalk\n\n\n\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\tVariants\n\t\t\t\t\t\t\n\n\t\t\t\t\t\t\n\n\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\tViews\n\n\t\t\t\t\t\t\tRead\n\tEdit\n\tView history\n\n\n\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\tMore\n\n\t\t\t\t\t\t\n\n\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\n\t\t\t\t\t\t\tSearch\n\t\t\t\t\t\t\n\n\t\t\t\t\t\t\n\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\t\t\t\t\n\n\t\t\t\t\t\t\n\n\t\t\t\t\t\n\n\t\t\t\t\t\t\t\t\t\n\n\t\t\t\n\n\t\t\t\n\t\t\t\t\n\n\t\t\t\t\t\t\n\t\t\tNavigation\n\n\t\t\t\n\t\t\t\t\t\t\t\t\tMain page\n\tContents\n\tFeatured content\n\tCurrent events\n\tRandom article\n\tDonate to Wikipedia\n\tWikipedia store\n\n\n\t\t\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\tInteraction\n\n\t\t\t\n\t\t\t\t\t\t\t\t\tHelp\n\tAbout Wikipedia\n\tCommunity portal\n\tRecent changes\n\tContact page\n\n\n\t\t\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\tTools\n\n\t\t\t\n\t\t\t\t\t\t\t\t\tWhat links here\n\tRelated changes\n\tUpload file\n\tSpecial pages\n\tPermanent link\n\tPage information\n\tWikidata item\n\tCite this page\n\n\n\t\t\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\tIn other projects\n\n\t\t\t\n\t\t\t\t\t\t\t\t\tWikimedia Commons\n\n\n\t\t\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\tPrint/export\n\n\t\t\t\n\t\t\t\t\t\t\t\t\tCreate a book\n\tDownload as PDF\n\tPrintable version\n\n\n\t\t\t\t\t\t\t\n\n\t\t\n\n\t\t\t\n\t\t\tLanguages\n\n\t\t\t\n\t\t\t\t\t\t\t\t\t\u0627\u0644\u0639\u0631\u0628\u064a\u0629\n\t\u09ac\u09be\u0982\u09b2\u09be\n\t\u0411\u044a\u043b\u0433\u0430\u0440\u0441\u043a\u0438\n\tCatal\u00e0\n\t\u010ce\u0161tina\n\tDansk\n\tDeutsch\n\tEesti\n\tEspa\u00f1ol\n\t\u0641\u0627\u0631\u0633\u06cc\n\tFran\u00e7ais\n\t\ud55c\uad6d\uc5b4\n\t\u0939\u093f\u0928\u094d\u0926\u0940\n\tBahasa Indonesia\n\tItaliano\n\t\u05e2\u05d1\u05e8\u05d9\u05ea\n\tLatvie\u0161u\n\tMagyar\n\t\u0d2e\u0d32\u0d2f\u0d3e\u0d33\u0d02\n\tBahasa Melayu\n\tNederlands\n\t\u65e5\u672c\u8a9e\n\tNorsk\n\tNorsk nynorsk\n\t\u0a2a\u0a70\u0a1c\u0a3e\u0a2c\u0a40\n\tPlattd\u00fc\u00fctsch\n\tPolski\n\tPortugu\u00eas\n\tRom\u00e2n\u0103\n\t\u0420\u0443\u0441\u0441\u043a\u0438\u0439\n\tSimple English\n\tSloven\u010dina\n\tSloven\u0161\u010dina\n\tSuomi\n\tSvenska\n\tT\u00fcrk\u00e7e\n\t\u0423\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430\n\t\u4e2d\u6587\n\n\n\t\t\t\tEdit links\n\t\t\t\n\n\t\t\n\n\t\t\t\t\n\n\t\t\n\n\t\t\t\t\n\t\t\t\t\t\t\t This page was last edited on 18 September 2019, at 12:06\u00a0(UTC).\n\tText is available under the Creative Commons Attribution-ShareAlike License;\nadditional terms may apply.  By using this site, you agree to the Terms of Use and Privacy Policy. Wikipedia\u00ae is a registered trademark of the Wikimedia Foundation, Inc., a non-profit organization.\n\n\n\t\t\t\t\t\t\tPrivacy policy\n\tAbout Wikipedia\n\tDisclaimers\n\tContact Wikipedia\n\tDevelopers\n\tCookie statement\n\tMobile view\n\n\n\t\t\t\t\t\t\t\t\t\t\t\n\t\t\t\t\t\t\t\t\t\t\t\n\t\n\t\t\t\t\t\t\t\t\t\t\t\n\n\n\t\t\t\t\t\t\n\n\t\t\n\n\t\t\n\n\n\n\n", "metadata": {"Content-Encoding": "UTF-8", "Content-Language": "en", "Content-Type": "text/html; charset=UTF-8", "ResourceLoaderDynamicStyles": "", "X-Parsed-By": ["org.apache.tika.parser.DefaultParser", "org.apache.tika.parser.html.HtmlParser"], "X-TIKA:parse_time_millis": "4", "dc:title": "Digital signal processor - Wikipedia", "generator": "MediaWiki 1.34.0-wmf.24", "og:image": "https://upload.wikimedia.org/wikipedia/commons/thumb/a/a9/Dsp_chip.jpg/1200px-Dsp_chip.jpg", "referrer": ["origin", "origin-when-crossorigin", "origin-when-cross-origin"], "resourceName": "https-en-wikipedia-org-wiki-digital_signal_processor", "title": "Digital signal processor - Wikipedia"}}